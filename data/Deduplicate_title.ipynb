{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplicate titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from simhash import Simhash, SimhashIndex\n",
    "\n",
    "data_path = '../data/'\n",
    "model_path = '../model/'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_webhouse_data(data_name): \n",
    "    data_file = data_path + data_name + '.json'\n",
    "    with open(data_file) as json_data:\n",
    "        data = json.load(json_data)\n",
    "        return data\n",
    "\n",
    "def cleanup_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = text.replace(\"'s\", \" \")\n",
    "    text = text.replace(\"n't\", \" not \")\n",
    "    text = text.replace(\"'ve\", \" have \")\n",
    "    text = text.replace(\"'re\", \" are \")\n",
    "    text = text.replace(\"I'm\",\" I am \")\n",
    "    text = text.replace(\"you're\",\" you are \")\n",
    "    text = text.replace(\"You're\",\" You are \")\n",
    "    text = text.replace(\"-\",\" \")\n",
    "    text = text.replace(\"/\",\" \")\n",
    "    text = text.replace(\"(\",\" \")\n",
    "    text = text.replace(\")\",\" \")\n",
    "    text = text.replace(\"%\",\" percent \")\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "    return text\n",
    "\n",
    "def load_wordvec_model(modelName, modelFile, flagBin):\n",
    "    model = KeyedVectors.load_word2vec_format(model_path + modelFile, binary=flagBin)\n",
    "    return model\n",
    "\n",
    "# data = load_webhouse_data('microsoft')\n",
    "data = load_webhouse_data('microsoft_0504_0604')\n",
    "model_w2v_AP = load_wordvec_model('Word2Vec Google News', 'GoogleNews-vectors-negative300.bin.gz', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get titles, cleanup text, and apply Simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_title = len(data)\n",
    "title_list = [cleanup_text(str(data[i]['title'])) for i in range(tot_title)]\n",
    "objs = [(i, Simhash(title_list[i])) for i in range(tot_title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function checks whether the input words are present in the vocabulary for the model\n",
    "def vocab_check(vectors, words):\n",
    "    output = list()\n",
    "    for word in words:\n",
    "        if word in vectors.vocab:\n",
    "            output.append(word.strip())\n",
    "    return output\n",
    "\n",
    "# function calculates similarity between two strings using a particular word vector model\n",
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    \n",
    "    output = vectors.n_similarity(s1words, s2words)\n",
    "    return output\n",
    "\n",
    "def remove_duplicate(index, title_list, index_simahs, model_w2v_AP):\n",
    "    duplicates = set()\n",
    "    title = title_list[index]\n",
    "    \n",
    "    # calculate  hash value\n",
    "    title_hash = Simhash(title)\n",
    "    \n",
    "    # find all duplicate indices\n",
    "    dup_indices = index_simahs.get_near_dups(title_hash)\n",
    "\n",
    "    # apply word2vec \n",
    "    for dupi in dup_indices:\n",
    "        if int(dupi) == index: \n",
    "            continue\n",
    "        try:\n",
    "            score = calc_similarity(title, title_list[int(dupi)], model_w2v_AP)\n",
    "        except:\n",
    "            score = 0\n",
    "        if score > 0.55:\n",
    "            duplicates.add(int(dupi))\n",
    "    return duplicates    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: \n",
      "San Francisco to vote on banning face recognition technology\n",
      "\n",
      "Duplicates:\n",
      "San Francisco to vote on banning face recognition technology\n",
      "San Francisco to vote on banning face recognition technology\n"
     ]
    }
   ],
   "source": [
    "distance = 15\n",
    "index_simahs = SimhashIndex(objs, k=distance)\n",
    "\n",
    "index = 2231\n",
    "print('Title: \\n' + title_list[index] + '\\n\\nDuplicates:')\n",
    "for i in remove_duplicate(index, title_list, index_simahs, model_w2v_AP): \n",
    "    print(title_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/7945 868 111.96980700000003\n",
      "4000/7945 1623 212.04117\n",
      "6000/7945 2277 312.80631600000004\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.clock()\n",
    "duplicates = set()\n",
    "for index in range(tot_title): \n",
    "    if index in duplicates: \n",
    "        continue \n",
    "    new_duplicates = remove_duplicate(index, title_list, index_simahs, model_w2v_AP);\n",
    "    duplicates = duplicates.union(new_duplicates)\n",
    "    if index % 2000 == 0 and index != 0: \n",
    "        print(str(index) + '/' + str(tot_title), len(duplicates), time.clock() - start)\n",
    "    \n",
    "new_feeds = data.copy()\n",
    "for dup in sorted([int(dup) for dup in duplicates], reverse=True):\n",
    "    del new_feeds[dup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data(data_name, feeds): \n",
    "    data_file = data_path + data_name + '.json'\n",
    "    if os.path.isfile(data_file): \n",
    "        raise ValueError(\"{} file already exists\".format(data_file))\n",
    "    with open(data_file, 'w') as outfile:\n",
    "        json.dump(feeds, outfile)\n",
    "        \n",
    "    \n",
    "data = store_data('microsoft_0504_0604_clean', new_feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
