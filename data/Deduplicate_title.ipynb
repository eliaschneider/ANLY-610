{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplicate titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from simhash import Simhash, SimhashIndex\n",
    "\n",
    "data_path = '../data/'\n",
    "model_path = '../model/'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def load_webhouse_data(data_name): \n",
    "    data_file = data_path + data_name + '.json'\n",
    "    with open(data_file) as json_data:\n",
    "        data = json.load(json_data)\n",
    "        return data\n",
    "\n",
    "def cleanup_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = text.replace(\"'s\", \" \")\n",
    "    text = text.replace(\"n't\", \" not \")\n",
    "    text = text.replace(\"'ve\", \" have \")\n",
    "    text = text.replace(\"'re\", \" are \")\n",
    "    text = text.replace(\"I'm\",\" I am \")\n",
    "    text = text.replace(\"you're\",\" you are \")\n",
    "    text = text.replace(\"You're\",\" You are \")\n",
    "    text = text.replace(\"-\",\" \")\n",
    "    text = text.replace(\"/\",\" \")\n",
    "    text = text.replace(\"(\",\" \")\n",
    "    text = text.replace(\")\",\" \")\n",
    "    text = text.replace(\"%\",\" percent \")\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "    return text\n",
    "\n",
    "def load_wordvec_model(modelName, modelFile, flagBin):\n",
    "    model = KeyedVectors.load_word2vec_format(model_path + modelFile, binary=flagBin)\n",
    "    return model\n",
    "\n",
    "# data = load_webhouse_data('microsoft')\n",
    "data = load_webhouse_data('microsoft_0419_0519')\n",
    "model_w2v_AP = load_wordvec_model('Word2Vec Google News', 'GoogleNews-vectors-negative300.bin.gz', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get titles, cleanup text, and apply Simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_title = len(data)\n",
    "title_list = [cleanup_text(str(data[i]['title'])) for i in range(tot_title)]\n",
    "objs = [(i, Simhash(title_list[i])) for i in range(tot_title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function checks whether the input words are present in the vocabulary for the model\n",
    "def vocab_check(vectors, words):\n",
    "    output = list()\n",
    "    for word in words:\n",
    "        if word in vectors.vocab:\n",
    "            output.append(word.strip())\n",
    "    return output\n",
    "\n",
    "# function calculates similarity between two strings using a particular word vector model\n",
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    \n",
    "    output = vectors.n_similarity(s1words, s2words)\n",
    "    return output\n",
    "\n",
    "def remove_duplicate(index, title_list, index_simahs, model_w2v_AP):\n",
    "    duplicates = set()\n",
    "    title = title_list[index]\n",
    "    \n",
    "    # calculate  hash value\n",
    "    title_hash = Simhash(title)\n",
    "    \n",
    "    # find all duplicate indices\n",
    "    dup_indices = index_simahs.get_near_dups(title_hash)\n",
    "\n",
    "    # apply word2vec \n",
    "    for dupi in dup_indices:\n",
    "        if int(dupi) == index: \n",
    "            continue\n",
    "        try:\n",
    "            score = calc_similarity(title, title_list[int(dupi)], model_w2v_AP)\n",
    "        except:\n",
    "            score = 0\n",
    "        if score > 0.55:\n",
    "            duplicates.add(int(dupi))\n",
    "    return duplicates    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: \n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "\n",
      "Duplicates:\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle East  sustainability pioneer Beeah selects Johnson Controls and Microsoft for its Office of the Future\n",
      "Middle East  sustainability pioneer Beeah selects Johnson Controls and Microsoft for its Office of the Future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its Office of the Future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle East  sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle East  sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle East  sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its Office of the Future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle East  sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts Sustainability Pioneer Beeah Selects Johnson Controls and Microsoft for its Office of the Future\n",
      "Microsoft  Middle East  sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future  MarketScreener\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle Easts sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle East  sustainability pioneer Beeah selects Johnson Controls and Microsoft for its office of the future\n",
      "Middle East  sustainability pioneer Beeah selects Johnson Controls and Microsoft for its Office of the Future\n"
     ]
    }
   ],
   "source": [
    "distance = 15\n",
    "index_simahs = SimhashIndex(objs, k=distance)\n",
    "\n",
    "index = 2231\n",
    "print('Title: \\n' + title_list[index] + '\\n\\nDuplicates:')\n",
    "for i in remove_duplicate(index, title_list, index_simahs, model_w2v_AP): \n",
    "    print(title_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/24140 1741 362.4728809999997\n",
      "6000/24140 3895 1071.9081139999998\n",
      "10000/24140 5262 1793.492303\n",
      "12000/24140 6049 2087.508836\n",
      "14000/24140 6806 2402.7792899999995\n",
      "18000/24140 7773 3063.233639\n",
      "22000/24140 8959 3662.3776609999995\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "duplicates = set()\n",
    "for index in range(tot_title): \n",
    "    if index in duplicates: \n",
    "        continue \n",
    "    new_duplicates = remove_duplicate(index, title_list, index_simahs, model_w2v_AP);\n",
    "    duplicates = duplicates.union(new_duplicates)\n",
    "    if index % 2000 == 0 and index != 0: \n",
    "        print(str(index) + '/' + str(tot_title), len(duplicates), time.clock() - start)\n",
    "    \n",
    "new_feeds = data.copy()\n",
    "for dup in sorted([int(dup) for dup in duplicates], reverse=True):\n",
    "    del new_feeds[dup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data(data_name, feeds): \n",
    "    data_file = data_path + data_name + '.json'\n",
    "    if os.path.isfile(data_file): \n",
    "        raise ValueError(\"{} file already exists\".format(data_file))\n",
    "    with open(data_file, 'w') as outfile:\n",
    "        json.dump(feeds, outfile)\n",
    "        \n",
    "    \n",
    "data = store_data('microsoft_clean', new_feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
