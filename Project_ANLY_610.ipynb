{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data from webhose.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import webhoseio\n",
    "\n",
    "data_path = './data'\n",
    "model_path = './model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful function to write and read json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data(filename, data): \n",
    "    data_file = os.path.join(data_path, filename)\n",
    "    with open(data_file, 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "def load_data(filename): \n",
    "    data_file = os.path.join(data_path, filename)\n",
    "    with open(data_file) as json_data:\n",
    "        data = json.load(json_data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query to webhose.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "webhoseio.config(token=\"1a815770-8785-4596-a11c-09bdae034336\")\n",
    "query_params = {\n",
    "    \"q\": \"language:english site_type:news site_category:media organization:Microsoft\", \n",
    "    \"ts\": \"1555697996143\",\n",
    "    \"sort\": \"crawled\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = webhoseio.query(\"filterWebContent\", query_params)\n",
    "feeds = [ item for item in output['posts']]\n",
    "while output['moreResultsAvailable'] > 0:\n",
    "    output = webhoseio.get_next()\n",
    "    feeds += [ item for item in output['posts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data('microsoft_0504_0604.json', feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Total number of news articles: 7948\n",
      "* Date range: from 2019-05-04T03:00:00.000+03:00 to 2019-06-04T08:41:00.000+03:00\n"
     ]
    }
   ],
   "source": [
    "news_list = load_data('microsoft_0504_0604.json')\n",
    "print('* Total number of news articles: {}'.format(len(news_list)))\n",
    "print('* Date range: from {} to {}'.format(\n",
    "    min([news_list[x]['published'] for x in range(len(news_list))]), \n",
    "    max([news_list[x]['published'] for x in range(len(news_list))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deduplicate titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from simhash import Simhash, SimhashIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful function to cleanup text and load word2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = text.replace(\"'s\", \" \")\n",
    "    text = text.replace(\"n't\", \" not \")\n",
    "    text = text.replace(\"'ve\", \" have \")\n",
    "    text = text.replace(\"'re\", \" are \")\n",
    "    text = text.replace(\"I'm\",\" I am \")\n",
    "    text = text.replace(\"you're\",\" you are \")\n",
    "    text = text.replace(\"You're\",\" You are \")\n",
    "    text = text.replace(\"-\",\" \")\n",
    "    text = text.replace(\"/\",\" \")\n",
    "    text = text.replace(\"(\",\" \")\n",
    "    text = text.replace(\")\",\" \")\n",
    "    text = text.replace(\"%\",\" percent \")\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "    text = \" \".join([i for i in text.lower().split() if i not in stopwords])\n",
    "    token = [WordNetLemmatizer().lemmatize(i) for i in text.split()]\n",
    "    return token\n",
    "\n",
    "def load_wordvec_model(modelName, modelFile, flagBin):\n",
    "    model = KeyedVectors.load_word2vec_format(os.path.join(model_path, modelFile), binary=flagBin)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word2vec and webhose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_AP = load_wordvec_model('Word2Vec Google News', 'GoogleNews-vectors-negative300.bin.gz', True)\n",
    "data = load_data('microsoft_0504_0604.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get titles, cleanup text, and apply Simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_title = len(data)\n",
    "title_list = [' '.join(cleanup_text(str(data[i]['title']))) for i in range(tot_title)]\n",
    "objs = [(i, Simhash(title_list[i])) for i in range(tot_title)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function checks whether the input words are present in the vocabulary for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_check(vectors, words):\n",
    "    output = list()\n",
    "    for word in words:\n",
    "        if word in vectors.vocab:\n",
    "            output.append(word.strip())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function calculates similarity between two strings using a particular word vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    \n",
    "    output = vectors.n_similarity(s1words, s2words)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: \n",
      "Chinese Military Ditching Microsoft Windows To Avoid CIA's 'Hefty Arsenal Of Hacking Tools'\n",
      "\n",
      "Duplicates:\n",
      "Chinese Military Ditching Microsoft Windows To Avoid CIA’s ‘Hefty Arsenal Of Hacking Tools’\n",
      "Chinese Military Ditching Microsoft Windows to Avoid CIA's Arsenal of Hacking Tools\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicate(index, title_list, index_simahs, model_w2v_AP):\n",
    "    duplicates = set()\n",
    "    title = title_list[index]\n",
    "    \n",
    "    # calculate  hash value\n",
    "    title_hash = Simhash(title)\n",
    "    \n",
    "    # find all duplicate indices\n",
    "    dup_indices = index_simahs.get_near_dups(title_hash)\n",
    "\n",
    "    # apply word2vec \n",
    "    for dupi in dup_indices:\n",
    "        if int(dupi) == index: \n",
    "            continue\n",
    "        try:\n",
    "            score = calc_similarity(title, title_list[int(dupi)], model_w2v_AP)\n",
    "        except:\n",
    "            score = 0\n",
    "        if score > 0.55:\n",
    "            duplicates.add(int(dupi))\n",
    "    return duplicates \n",
    "\n",
    "distance = 15\n",
    "index_simahs = SimhashIndex(objs, k=distance)\n",
    "\n",
    "index = 6241\n",
    "print('Title: \\n' + data[index]['title'] + '\\n\\nDuplicates:')\n",
    "for i in remove_duplicate(index, title_list, index_simahs, model_w2v_AP): \n",
    "    print(data[i]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/7948 1036 141.879877\n",
      "4000/7948 1819 251.23051999999998\n",
      "6000/7948 2471 364.603786\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "duplicates = set()\n",
    "for index in range(tot_title): \n",
    "    if index in duplicates: \n",
    "        continue \n",
    "    new_duplicates = remove_duplicate(index, title_list, index_simahs, model_w2v_AP);\n",
    "    duplicates = duplicates.union(new_duplicates)\n",
    "    if index % 2000 == 0 and index != 0: \n",
    "        print(str(index) + '/' + str(tot_title), len(duplicates), time.clock() - start)\n",
    "    \n",
    "new_feeds = data.copy()\n",
    "for dup in sorted([int(dup) for dup in duplicates], reverse=True):\n",
    "    del new_feeds[dup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data('microsoft_0504_0604_clean.json', new_feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Total number of news articles once removed duplicates: 5021\n",
      "* Date range: from 2019-05-04T03:00:00.000+03:00 to 2019-05-23T18:57:00.000+03:00\n"
     ]
    }
   ],
   "source": [
    "news_cleaned_list = load_data('microsoft_0504_0604_clean.json')\n",
    "print('* Total number of news articles once removed duplicates: {}'.format(len(news_cleaned_list)))\n",
    "print('* Date range: from {} to {}'.format(\n",
    "    min([news_list[x]['published'] for x in range(len(news_cleaned_list))]), \n",
    "    max([news_list[x]['published'] for x in range(len(news_cleaned_list))])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition using IBM Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 import Features, EntitiesOptions, KeywordsOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('microsoft_0504_0604_clean.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query to IBM Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: NaturalLanguageUnderstandingV1 is a deprecated function. watson-developer-cloud moved to ibm-watson\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2018-11-16',\n",
    "    iam_apikey='N9R1KBh-8CJVJWSMGHB_lLG-Sq03-BurNjDc_dh5l9Id',\n",
    "    url='https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2018-11-16'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBSE 10th Result 2019: Candidates can use alternative methods to provided by Google, Microsoft if official website is down\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Company': 'Microsoft', 'JobTitle': 'official', 'Organization': 'CBSE'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_title_WATSON_entities(text):\n",
    "    entities_dict = dict()\n",
    "    r = natural_language_understanding.analyze(\n",
    "        text=text, features=Features(entities=EntitiesOptions(sentiment=True, limit=10))).get_result()\n",
    "    for entity in r['entities']:\n",
    "        entities_dict[entity['type']] = entity['text']\n",
    "    return entities_dict\n",
    "\n",
    "title = data[11]['title']\n",
    "print(title)\n",
    "get_title_WATSON_entities(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)): \n",
    "    title = data[i]['title']\n",
    "    try:\n",
    "        entities = get_title_WATSON_entities(title)\n",
    "    except Exception:\n",
    "        entities = dict()\n",
    "    data[i]['title_entities'] = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_data('microsoft_0504_0604_clean_with_entities.json', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Total number of news entities: 27\n",
      "\n",
      "* List of entities find in titles:\n",
      " -Person\n",
      " -Company\n",
      " -JobTitle\n",
      " -Location\n",
      " -GeographicFeature\n",
      " -Organization\n",
      " -Quantity\n",
      " -PrintMedia\n",
      " -Facility\n",
      " -Sport\n",
      " -Broadcaster\n",
      " -Number\n",
      " -Drug\n",
      " -Hashtag\n",
      " -HealthCondition\n",
      " -TwitterHandle\n",
      " -IPAddress\n",
      " -Movie\n",
      " -Date\n",
      " -Crime\n",
      " -Measure\n",
      " -MusicGroup\n",
      " -Money\n",
      " -TelevisionShow\n",
      " -Vehicle\n",
      " -Award\n",
      " -Percent\n",
      "\n",
      "* List of words associated with Broadcaster:\n",
      " -ABC\n",
      " -BBC News\n",
      " -HBO\n",
      " -CNN\n",
      " -NBC\n",
      " -KTLA\n",
      " -CBS\n",
      " -FOX News\n",
      " -Fox News\n",
      " -TMZ\n"
     ]
    }
   ],
   "source": [
    "news_cleaned_list = load_data('microsoft_0504_0604_clean_with_entities.json')\n",
    "entity_title_list = [news['title_entities'] for news in news_cleaned_list if len(news['title_entities'].keys()) > 0]\n",
    "tot_entities_dict = dict()\n",
    "for entity_title in entity_title_list: \n",
    "    for entity in entity_title:\n",
    "        if entity in tot_entities_dict.keys():\n",
    "            tot_entities_dict[entity] += [entity_title[entity]]\n",
    "        else:\n",
    "            tot_entities_dict[entity] = [entity_title[entity]]\n",
    "print('* Total number of news entities: {}'.format(len(tot_entities_dict)) + '\\n')\n",
    "print('* List of entities find in titles:\\n -' + '\\n -'.join(list(tot_entities_dict.keys())) + '\\n')\n",
    "print('* List of words associated with Broadcaster:\\n -' + '\\n -'.join(list(set(tot_entities_dict['Broadcaster']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic cluster using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import ldamodel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [[data[i]['title'], data[i]['published'][:10]]  for i in range(len(data))]\n",
    "df_feeds = pd.DataFrame(titles,columns=['title', 'date'])\n",
    "titles = df_feeds[['title']].applymap(cleanup_text)['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(titles)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.8)\n",
    "corpora = [dictionary.doc2bow(doc) for doc in titles]\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "lda_model = ldamodel.LdaModel(corpora, num_topics=7, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpora, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/41819761/pyldavis-visualization-of-pyspark-generated-lda-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
